{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6be383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metadata = pd.read_csv(\"E:\\Hakaton\\participants\\participants\\data\\metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33383058",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"timestamp\"] = pd.to_datetime(metadata[\"timestamp\"])\n",
    "metadata = metadata.sort_values(\"timestamp\")\n",
    "split_date = pd.Timestamp(\"2025-04-01\")\n",
    "train_df = metadata[metadata[\"timestamp\"] < split_date]\n",
    "valid_df = metadata[metadata[\"timestamp\"] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d963388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего книг в valid: 10339\n",
      "Холодных книг в valid: 3185\n",
      "Доля холодных: 0.3080568720379147\n"
     ]
    }
   ],
   "source": [
    "train_items = set(train_df[\"edition_id\"].unique())\n",
    "valid_items = set(valid_df[\"edition_id\"].unique())\n",
    "cold_items = valid_items - train_items\n",
    "print(\"Всего книг в valid:\", len(valid_items))\n",
    "print(\"Холодных книг в valid:\", len(cold_items))\n",
    "print(\"Доля холодных:\", len(cold_items) / len(valid_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a4e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop = (\n",
    "    train_df\n",
    "    .groupby(\"edition_id\")\n",
    "    .size()\n",
    "    .rename(\"item_popularity\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "train_df = train_df.merge(item_pop, on=\"edition_id\", how=\"left\")\n",
    "valid_df = valid_df.merge(item_pop, on=\"edition_id\", how=\"left\")\n",
    "train_df[\"item_popularity\"] = train_df[\"item_popularity\"].fillna(0)\n",
    "valid_df[\"item_popularity\"] = valid_df[\"item_popularity\"].fillna(0)\n",
    "\n",
    "user_genre_cnt = (\n",
    "    train_df\n",
    "    .groupby([\"user_id\", \"genre_id\"])\n",
    "    .size()\n",
    "    .rename(\"user_genre_cnt\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_total = (\n",
    "    train_df\n",
    "    .groupby(\"user_id\")\n",
    "    .size()\n",
    "    .rename(\"user_total\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_genre_freq = user_genre_cnt.merge(user_total, on=\"user_id\")\n",
    "user_genre_freq[\"user_genre_freq\"] = (\n",
    "    user_genre_freq[\"user_genre_cnt\"] / user_genre_freq[\"user_total\"]\n",
    ")\n",
    "\n",
    "user_genre_freq = user_genre_freq[\n",
    "    [\"user_id\", \"genre_id\", \"user_genre_freq\"]\n",
    "]\n",
    "train_df = train_df.merge(\n",
    "    user_genre_freq,\n",
    "    on=[\"user_id\", \"genre_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "valid_df = valid_df.merge(\n",
    "    user_genre_freq,\n",
    "    on=[\"user_id\", \"genre_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "train_df[\"user_genre_freq\"] = train_df[\"user_genre_freq\"].fillna(0)\n",
    "valid_df[\"user_genre_freq\"] = valid_df[\"user_genre_freq\"].fillna(0)\n",
    "user_author_seen = (\n",
    "    train_df\n",
    "    .groupby([\"user_id\", \"author_id\"])\n",
    "    .size()\n",
    "    .rename(\"user_author_seen\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_author_seen[\"user_author_seen\"] = 1\n",
    "train_df = train_df.merge(\n",
    "    user_author_seen,\n",
    "    on=[\"user_id\", \"author_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "valid_df = valid_df.merge(\n",
    "    user_author_seen,\n",
    "    on=[\"user_id\", \"author_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "train_df[\"user_author_seen\"] = train_df[\"user_author_seen\"].fillna(0)\n",
    "valid_df[\"user_author_seen\"] = valid_df[\"user_author_seen\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4992f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 434696\n",
      "valid: 30048\n",
      "2024-10-14 18:48:56 2025-03-31 23:59:46\n",
      "2025-04-01 00:00:55 2025-04-12 12:44:23\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", len(train_df))\n",
    "print(\"valid:\", len(valid_df))\n",
    "\n",
    "print(train_df[\"timestamp\"].min(), train_df[\"timestamp\"].max())\n",
    "print(valid_df[\"timestamp\"].min(), valid_df[\"timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FULL-VIBECODE функция генерации негативов\n",
    "Ебаное говно, надо исправлять 100% и самим разбираться\n",
    "как делаются негативы, я просто заебался пока сидел воял это\n",
    "\n",
    "TODO:\n",
    "Переделать генерацию негативов полностью самостоятельно\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_stores(train_pos_df):\n",
    "    item_store_cols = [\n",
    "        \"edition_id\",\"rating\",\"book_id\",\"author_id\",\"publication_year\",\"age_restriction\",\n",
    "        \"language_id\",\"publisher_id\",\"title\",\"description\",\"genre_id\",\"genre_name\",\n",
    "        \"author_name\",\"book_age_years\",\"item_popularity\"\n",
    "    ]\n",
    "    item_store = train_pos_df[item_store_cols].drop_duplicates(\"edition_id\")\n",
    "\n",
    "    user_store = train_pos_df[[\"user_id\",\"gender\",\"age\"]].drop_duplicates(\"user_id\")\n",
    "\n",
    "    user_genre_store = train_pos_df[[\"user_id\",\"genre_id\",\"user_genre_freq\"]].drop_duplicates([\"user_id\",\"genre_id\"])\n",
    "\n",
    "    user_author_store = train_pos_df[[\"user_id\",\"author_id\",\"user_author_seen\"]].drop_duplicates([\"user_id\",\"author_id\"])\n",
    "\n",
    "    return item_store, user_store, user_genre_store, user_author_store\n",
    "\n",
    "\n",
    "def gen_negs(pos_df, item_pool, n_neg=50, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pool = np.array(list(item_pool))\n",
    "\n",
    "    seen = pos_df.groupby(\"user_id\")[\"edition_id\"].apply(set).to_dict()\n",
    "    users = pos_df[\"user_id\"].unique()\n",
    "\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        seen_items = seen.get(u, set())\n",
    "        candidates = pool[~np.isin(pool, list(seen_items))]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        k = min(n_neg, len(candidates))\n",
    "        neg_items = rng.choice(candidates, size=k, replace=False)\n",
    "        rows.extend([(u, int(it)) for it in neg_items])\n",
    "\n",
    "    neg = pd.DataFrame(rows, columns=[\"user_id\",\"edition_id\"])\n",
    "    neg[\"event_type\"] = 0\n",
    "    neg[\"timestamp\"] = pd.NaT\n",
    "    neg[\"year\"] = 0\n",
    "    neg[\"day\"] = 0\n",
    "    neg[\"rating\"] = np.nan\n",
    "    return neg\n",
    "\n",
    "\n",
    "def attach_features(neg_df, item_store, user_store, user_genre_store, user_author_store, age_fill, gender_fill=0):\n",
    "    neg_df = neg_df.merge(item_store, on=\"edition_id\", how=\"left\")\n",
    "    neg_df = neg_df.merge(user_store, on=\"user_id\", how=\"left\")\n",
    "    neg_df = neg_df.merge(user_genre_store, on=[\"user_id\",\"genre_id\"], how=\"left\")\n",
    "    neg_df = neg_df.merge(user_author_store, on=[\"user_id\",\"author_id\"], how=\"left\")\n",
    "\n",
    "    neg_df[\"item_popularity\"] = neg_df[\"item_popularity\"].fillna(0)\n",
    "    neg_df[\"user_genre_freq\"] = neg_df[\"user_genre_freq\"].fillna(0)\n",
    "    neg_df[\"user_author_seen\"] = neg_df[\"user_author_seen\"].fillna(0)\n",
    "\n",
    "    neg_df[\"age\"] = neg_df[\"age\"].fillna(age_fill)\n",
    "    neg_df[\"gender\"] = neg_df[\"gender\"].fillna(gender_fill)\n",
    "\n",
    "    neg_df[\"description\"] = neg_df[\"description\"].fillna(\"Нет описания\")\n",
    "    return neg_df\n",
    "\n",
    "\n",
    "\n",
    "train_pos_df, valid_pos_df = train_df.copy(), valid_df.copy()\n",
    "item_store, user_store, user_genre_store, user_author_store = build_stores(train_pos_df)\n",
    "age_fill = train_pos_df[\"age\"].median()\n",
    "\n",
    "train_item_pool = set(train_pos_df[\"edition_id\"].unique())\n",
    "all_item_pool = set(metadata[\"edition_id\"].unique())\n",
    "\n",
    "train_neg = gen_negs(train_pos_df, train_item_pool, n_neg=50, seed=1)\n",
    "train_neg = attach_features(train_neg, item_store, user_store, user_genre_store, user_author_store, age_fill)\n",
    "train_df = pd.concat([train_pos_df, train_neg], ignore_index=True)\n",
    "\n",
    "valid_neg = gen_negs(valid_pos_df, all_item_pool, n_neg=50, seed=2)\n",
    "valid_neg = attach_features(valid_neg, item_store, user_store, user_genre_store, user_author_store, age_fill)\n",
    "valid_df = pd.concat([valid_pos_df, valid_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fbf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "valid_df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d2d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"rating\"], inplace=True)\n",
    "valid_df.drop(columns=[\"rating\"], inplace=True)\n",
    "\n",
    "train_df[\"description\"] = train_df[\"description\"].fillna(\"Нет описания\")\n",
    "valid_df[\"description\"] = valid_df[\"description\"].fillna(\"Нет описания\")\n",
    "\n",
    "train_df.drop(columns=[\"author_name\"], inplace=True)\n",
    "valid_df.drop(columns=[\"author_name\"], inplace=True)\n",
    "\n",
    "train_df[\"age\"] = train_df[\"age\"].fillna(train_df[\"age\"].median())\n",
    "valid_df[\"age\"] = valid_df[\"age\"].fillna(train_df[\"age\"].median())\n",
    "\n",
    "train_df[\"gender\"] = train_df[\"gender\"].fillna(0)\n",
    "valid_df[\"gender\"] = valid_df[\"gender\"].fillna(0)\n",
    "\n",
    "train_df.drop(columns=[\"rating_x\", \"rating_y\"], inplace=True)\n",
    "valid_df.drop(columns=[\"rating_x\", \"rating_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a704a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = metadata[[\"edition_id\", \"title\", \"description\"]].drop_duplicates(\"edition_id\").copy()\n",
    "items[\"text\"] = (items[\"title\"].fillna(\"\") + \" \" + items[\"description\"].fillna(\"\")).str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9016ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_items = set(train_df[\"edition_id\"].unique())\n",
    "items_train = items[items[\"edition_id\"].isin(train_items)].copy()\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=200_000,      # можно 50k–300k\n",
    "    ngram_range=(1, 2),        # униграммы+биграммы\n",
    "    min_df=2,                  # игнор редких\n",
    "    max_df=0.9,                # игнор слишком частых\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    ")\n",
    "\n",
    "X_tfidf_train = tfidf.fit_transform(items_train[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f11885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_svd_train = svd.fit_transform(X_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c73ae66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_all = tfidf.transform(items[\"text\"])\n",
    "X_svd_all = svd.transform(X_tfidf_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2faf14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "svd_cols = [f\"tfidf_svd_{i}\" for i in range(X_svd_all.shape[1])]\n",
    "item_text_df = pd.DataFrame(X_svd_all, columns=svd_cols)\n",
    "item_text_df.insert(0, \"edition_id\", items[\"edition_id\"].values)\n",
    "\n",
    "train_df = train_df.merge(item_text_df, on=\"edition_id\", how=\"left\")\n",
    "valid_df = valid_df.merge(item_text_df, on=\"edition_id\", how=\"left\")\n",
    "\n",
    "train_df[svd_cols] = train_df[svd_cols].fillna(0.0)\n",
    "valid_df[svd_cols] = valid_df[svd_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccc8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"description\", \"title\"], inplace=True)\n",
    "valid_df.drop(columns=[\"description\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f40ee214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"genre_name\"], inplace=True)\n",
    "valid_df.drop(columns=[\"genre_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b13f092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"timestamp\", \"year\", \"day\"], inplace=True)\n",
    "valid_df.drop(columns=[\"timestamp\", \"year\", \"day\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab266ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ЗАМЕТКА:\n",
    "У многих книг пропущен publication_year (0) из за чего их возраст стал 2025 лет\n",
    "\n",
    "TODO:\n",
    "Поправить надо бы\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d496c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "valid_df.to_csv(\"validation_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29676b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Делаем датафреймы с фичами что бы потом на предикте\n",
    "юзать их. Полная хуйня в реализации, тут мб какая то ошибка есть\n",
    "\n",
    "TODO:\n",
    "Придумать что то получше для мерджа данных при предикте\n",
    "\"\"\"\n",
    "\n",
    "item_features = (\n",
    "    train_df[\n",
    "        [\"edition_id\",\n",
    "         \"book_id\", \"author_id\", \"genre_id\",\n",
    "         \"publication_year\", \"age_restriction\",\n",
    "         \"language_id\", \"publisher_id\",\n",
    "         \"book_age_years\",\n",
    "         \"item_popularity\"]\n",
    "        + [c for c in train_df.columns if c.startswith(\"tfidf_svd_\")]\n",
    "    ]\n",
    "    .drop_duplicates(\"edition_id\")\n",
    ")\n",
    "\n",
    "item_features.to_csv(\"item_features.csv\", index=False)\n",
    "\n",
    "user_features = (\n",
    "    train_df[\n",
    "        [\"user_id\", \"age\", \"gender\"]\n",
    "    ]\n",
    "    .drop_duplicates(\"user_id\")\n",
    ")\n",
    "\n",
    "user_features.to_csv(\"user_features.csv\", index=False)\n",
    "\n",
    "user_genre_freq = (\n",
    "    train_df[\n",
    "        [\"user_id\", \"genre_id\", \"user_genre_freq\"]\n",
    "    ]\n",
    "    .drop_duplicates([\"user_id\", \"genre_id\"])\n",
    ")\n",
    "\n",
    "user_genre_freq.to_csv(\"user_genre_freq.csv\", index=False)\n",
    "\n",
    "user_author_seen = (\n",
    "    train_df[\n",
    "        [\"user_id\", \"author_id\", \"user_author_seen\"]\n",
    "    ]\n",
    "    .drop_duplicates([\"user_id\", \"author_id\"])\n",
    ")\n",
    "\n",
    "user_author_seen.to_csv(\"user_author_seen.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTO_GYM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
